#!/usr/bin/env python3
"""
åŒ»å­¦AI Agentå¯åŠ¨è„šæœ¬
æ·»åŠ çœŸæ­£çš„LLM APIè°ƒç”¨åŠŸèƒ½å’Œå‘é‡åŒ–embeddingå¤„ç†
"""

from fastapi import FastAPI, HTTPException, Form, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
import json
import uvicorn
import requests
import os
import shutil
from pathlib import Path
import numpy as np
import chardet
#!/usr/bin/env python3
import logging
import sys
from datetime import datetime

# å¯¼å…¥çœŸå®åè®®ç”Ÿæˆå™¨
try:
    from real_protocol_generator import RealProtocolGenerator
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥çœŸå®åè®®ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    RealProtocolGenerator = None

# é¦–å…ˆé…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)

# æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
for handler in root_logger.handlers[:]:
    root_logger.removeHandler(handler)

# åˆ›å»ºæ–°çš„å¤„ç†å™¨
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)

# è®¾ç½®æ ¼å¼
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
console_handler.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨åˆ°æ ¹æ—¥å¿—è®°å½•å™¨
root_logger.addHandler(console_handler)

# åˆ›å»ºåº”ç”¨ç‰¹å®šçš„logger
logger = logging.getLogger("medical_ai_agent")
logger.setLevel(logging.INFO)

# æµ‹è¯•æ—¥å¿—æ˜¯å¦å·¥ä½œ
logger.info("ğŸš€ æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
print("å¦‚æœä½ çœ‹åˆ°è¿™è¡Œä½†çœ‹ä¸åˆ°ä¸Šé¢çš„æ—¥å¿—ï¼Œè¯´æ˜æ—¥å¿—é…ç½®æœ‰é—®é¢˜", flush=True)


# æ·»åŠ æ™ºèƒ½æ–‡ä»¶ç¼–ç æ£€æµ‹å‡½æ•°
def read_file_with_encoding_detection(file_path):
    """æ™ºèƒ½æ£€æµ‹æ–‡ä»¶ç¼–ç å¹¶è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        # 1. é¦–å…ˆè¯»å–æ–‡ä»¶çš„åŸå§‹å­—èŠ‚æ•°æ®è¿›è¡Œç¼–ç æ£€æµ‹
        with open(file_path, 'rb') as f:
            raw_data = f.read()
        
        # 2. ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
        detected = chardet.detect(raw_data)
        confidence = detected.get('confidence', 0)
        encoding = detected.get('encoding', 'utf-8')
        logger.info(f"ğŸ“„ [ç¼–ç æ£€æµ‹] æ–‡ä»¶: {file_path.name}")
        logger.info(f"   æ£€æµ‹åˆ°ç¼–ç : {encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # 3. å¦‚æœç½®ä¿¡åº¦è¾ƒé«˜ï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç 
        if confidence > 0.7 and encoding:
            try:
                content = raw_data.decode(encoding)
                logger.info(f"   âœ… ä½¿ç”¨æ£€æµ‹ç¼–ç  {encoding} æˆåŠŸè¯»å–")
                return content
            except (UnicodeDecodeError, LookupError) as e:
                logger.warning(f"   âŒ æ£€æµ‹ç¼–ç  {encoding} è¯»å–å¤±è´¥: {e}")
        
        # 4. å¦‚æœæ£€æµ‹å¤±è´¥æˆ–ç½®ä¿¡åº¦ä¸é«˜ï¼Œå°è¯•å¸¸è§ç¼–ç 
        common_encodings = ['utf-8', 'gbk', 'gb2312', 'utf-16', 'utf-16le', 'utf-16be', 'latin1', 'cp1252']
        
        for enc in common_encodings:
            try:
                content = raw_data.decode(enc)
                logger.info(f"   âœ… ä½¿ç”¨å¤‡é€‰ç¼–ç  {enc} æˆåŠŸè¯»å–")
                return content
            except (UnicodeDecodeError, LookupError):
                continue
        
        # 5. å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨é”™è¯¯å¤„ç†æ–¹å¼
        try:
            content = raw_data.decode('utf-8', errors='replace')
            logger.warning(f"   âš ï¸ ä½¿ç”¨UTF-8é”™è¯¯æ›¿æ¢æ¨¡å¼è¯»å–")
            return content
        except Exception as e:
            logger.error(f"   âŒ æ‰€æœ‰ç¼–ç å°è¯•å¤±è´¥: {e}")
            return f"æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: ç¼–ç æ£€æµ‹å¤±è´¥"
    
    except Exception as e:
        logger.error(f"   ğŸ’¥ æ–‡ä»¶è¯»å–å¼‚å¸¸: {e}")
        return f"æ— æ³•è¯»å–åŸå§‹æ–‡ä»¶: {str(e)}"

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="åŒ»å­¦AI Agent - ä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ™ºèƒ½æ’°å†™API",
    description="ä¸“é—¨ç”¨äºæ¶æ€§è‚¿ç˜¤CGTé¢†åŸŸçš„ä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ™ºèƒ½æ’°å†™ç³»ç»Ÿ",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class ProtocolGenerationRequest(BaseModel):
    user_requirement: str
    model_type: str = "local"
    include_quality_check: bool = True
    include_literature: bool = True
    temperature: float = 0.3

class ChatRequest(BaseModel):
    message: str
    temperature: float = 0.3

# å…¨å±€é…ç½®
current_config = {
    "llm": {
        "type": "local",
        "url": "http://192.168.22.191:8000/v1",
        "model": "/home/aiteam/.cache/modelscope/hub/models/google/medgemma-27b-text-it/",
        "key": "EMPTY",
        "temperature": 0.3
    },
    "embedding": {
        "type": "local-api",
        "url": "http://192.168.196.151:9998/v1",
        "key": "EMPTY",
        "model": "bge-large-zh-v1.5",  # è‡ªåŠ¨è·å–ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        "dimension": 1024  # é»˜è®¤ç»´åº¦
    }
}

# å…¨å±€å˜é‡ - å‘é‡æ•°æ®åº“
embedded_documents = []  # å­˜å‚¨æ–‡æ¡£å’Œå¯¹åº”çš„embeddingå‘é‡
uploaded_files = []  # å­˜å‚¨ä¸Šä¼ çš„æ–‡ä»¶ä¿¡æ¯
knowledge_stats = {
    "ä¸´åºŠè¯•éªŒæ–¹æ¡ˆç¤ºä¾‹": {"document_count": 5},
    "è‚¿ç˜¤ä¸´åºŠæŒ‡å—": {"document_count": 8}, 
    "åŒ»å­¦æ–‡çŒ®": {"document_count": 12},
    "CGTè¯ç‰©ç ”å‘èµ„æ–™": {"document_count": 6},
    "Excelæ•°æ®è¡¨": {"document_count": 3},
    "ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£": {"document_count": 0}
}
generation_history = []

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

# å‘é‡ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°
def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    try:
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(v1, v2)
        norm_v1 = np.linalg.norm(v1)
        norm_v2 = np.linalg.norm(v2)
        
        if norm_v1 == 0 or norm_v2 == 0:
            return 0.0
        
        similarity = dot_product / (norm_v1 * norm_v2)
        return float(similarity)
    except Exception as e:
        logger.warning(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
        return 0.0

# çœŸæ­£çš„embeddingå‡½æ•°
def get_embedding(text: str) -> List[float]:
    """è°ƒç”¨é…ç½®çš„embedding APIè·å–æ–‡æœ¬å‘é‡"""
    try:
        if current_config["embedding"]["type"] == "local-api":
            headers = {
                "Authorization": f"Bearer {current_config['embedding']['key']}",
                "Content-Type": "application/json"
            }
            
            # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
            model_name = current_config['embedding']['model']
            if model_name == "auto":
                # å°è¯•è·å–ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
                try:
                    models_response = requests.get(
                        f"{current_config['embedding']['url']}/models",
                        headers=headers,
                        timeout=10
                    )
                    if models_response.status_code == 200:
                        models_data = models_response.json()
                        if models_data.get('data') and len(models_data['data']) > 0:
                            model_name = models_data['data'][0]['id']
                        elif models_data.get('models') and len(models_data['models']) > 0:
                            model_name = models_data['models'][0]['id']
                        else:
                            model_name = "text-embedding-ada-002"  # é»˜è®¤æ¨¡å‹
                except:
                    model_name = "text-embedding-ada-002"  # é™çº§é»˜è®¤
            
            data = {
                "model": model_name,
                "input": [text]
            }
            
            response = requests.post(
                f"{current_config['embedding']['url']}/embeddings",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                
                # å¤„ç†ä¸åŒçš„APIå“åº”æ ¼å¼
                if 'data' in result and len(result['data']) > 0:
                    if 'embedding' in result['data'][0]:
                        return result['data'][0]['embedding']
                elif 'embeddings' in result and len(result['embeddings']) > 0:
                    return result['embeddings'][0]
                elif 'embedding' in result:
                    return result['embedding']
                
                raise ValueError(f"æ— æ³•è§£æembeddingå“åº”: {result}")
            else:
                raise ValueError(f"Embedding APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
        else:
            # é™çº§åˆ°ç®€å•çš„mock embedding
            import hashlib
            # ä½¿ç”¨æ–‡æœ¬hashç”Ÿæˆå›ºå®šç»´åº¦çš„ä¼ªå‘é‡
            text_hash = hashlib.sha256(text.encode()).hexdigest()
            # ç”Ÿæˆ768ç»´å‘é‡ï¼ˆå¸¸è§çš„embeddingç»´åº¦ï¼‰
            fake_embedding = [float(int(text_hash[i:i+2], 16)) / 255.0 - 0.5 for i in range(0, min(len(text_hash), 128), 2)]
            # å¡«å……åˆ°768ç»´
            while len(fake_embedding) < 768:
                fake_embedding.extend(fake_embedding[:min(768-len(fake_embedding), len(fake_embedding))])
            return fake_embedding[:768]
    except Exception as e:
        logger.error(f"Embeddingç”Ÿæˆå¤±è´¥: {e}")
        logger.warning(f"Embeddingç”Ÿæˆå¤±è´¥: {e}")

def call_local_llm(message: str, temperature: float = 0.3) -> str:
    """è°ƒç”¨æœ¬åœ°LLMæ¨¡å‹"""
    try:
        # ç›´æ¥ä½¿ç”¨requestsè°ƒç”¨API
        headers = {
            "Authorization": f"Bearer {current_config['llm']['key']}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": current_config["llm"]["model"],
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·å¤„ç†ä¸´åºŠè¯•éªŒæ–¹æ¡ˆç›¸å…³çš„é—®é¢˜ã€‚è¯·ç”¨ä¸­æ–‡å›å¤ã€‚"},
                {"role": "user", "content": message}
            ],
            "temperature": temperature,
            "max_tokens": 1000
        }
        
        response = requests.post(
            f"{current_config['llm']['url']}/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["message"]["content"]
        else:
            return f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}"
            
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼ŒLLMè°ƒç”¨å¤±è´¥: {str(e)}"

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›APIä¿¡æ¯"""
    return {
        "message": "åŒ»å­¦AI Agent - ä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ™ºèƒ½æ’°å†™API",
        "version": "1.0.0",
        "docs": "/docs",
        "status": "running"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "medical-ai-agent"
    }

@app.get("/status")
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    return {
        "status": "healthy",
        "version": "1.0.0",
        "knowledge_base_status": {
            "status": "ready",
            "types_count": 10,
            "embedded_documents": len(embedded_documents)
        },
        "available_models": ["local", "openai", "deepseek"]
    }

@app.post("/test/llm")
async def test_llm_connection():
    """æµ‹è¯•LLMè¿æ¥"""
    try:
        # çœŸæ­£è°ƒç”¨LLMè¿›è¡Œæµ‹è¯•
        test_message = "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿æ¥æµ‹è¯•ã€‚è¯·ç®€çŸ­å›å¤ç¡®è®¤ä½ èƒ½æ­£å¸¸å·¥ä½œã€‚"
        response = call_local_llm(test_message, 0.3)
        
        return {
            "success": True,
            "message": "LLMè¿æ¥æµ‹è¯•æˆåŠŸ",
            "response": response,
            "model_config": {
                "url": current_config["llm"]["url"],
                "model": current_config["llm"]["model"]
            }
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"LLMè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"
        }

@app.post("/chat")
async def chat_with_llm(request: ChatRequest):
    """ä¸LLMå¯¹è¯"""
    try:
        response = call_local_llm(request.message, request.temperature)
        
        return {
            "success": True,
            "message": request.message,
            "response": response,
            "timestamp": datetime.now().isoformat(),
            "model_used": current_config["llm"]["model"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¹è¯å¤±è´¥: {str(e)}")

@app.post("/chat_stream")
async def chat_with_llm_stream(request: ChatRequest):
    """ä¸LLMå¯¹è¯ï¼Œæµå¼è¿”å›"""
    from fastapi.responses import StreamingResponse
    import asyncio

    async def generate_chat():
        try:
            response_text = call_local_llm(request.message, request.temperature)

            for chunk in chunk_text(response_text, chunk_size=50, overlap=0):
                yield f"data: {json.dumps({'content': chunk})}\n\n"
                await asyncio.sleep(0.02)

            yield "data: {\"done\": true}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e), 'done': True})}\n\n"

    return StreamingResponse(
        generate_chat(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )

@app.post("/test/embedding")
async def test_embedding_model():
    """æµ‹è¯•åµŒå…¥æ¨¡å‹"""
    try:
        if current_config["embedding"]["type"] == "local-api":
            try:
                # é¦–å…ˆè·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
                headers = {
                    "Authorization": f"Bearer {current_config['embedding']['key']}",
                    "Content-Type": "application/json"
                }
                
                # è·å–æ¨¡å‹åˆ—è¡¨
                try:
                    models_response = requests.get(
                        f"{current_config['embedding']['url']}/models",
                        headers=headers,
                        timeout=10
                    )
                    
                    model_name = current_config['embedding']['model']
                    if models_response.status_code == 200 and model_name == "auto":
                        models_data = models_response.json()
                        # æ£€æŸ¥è¿”å›çš„æ•°æ®ç»“æ„
                        if models_data.get('data') and len(models_data['data']) > 0:
                            model_name = models_data['data'][0]['id']
                        elif models_data.get('models') and len(models_data['models']) > 0:
                            # æœ‰äº›APIè¿”å›ä¸åŒçš„ç»“æ„
                            model_name = models_data['models'][0]['id']
                        else:
                            # å¦‚æœæ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹åç§°
                            model_name = current_config['embedding']['model']
                        
                        # æ›´æ–°é…ç½®ä¸­çš„æ¨¡å‹åç§°
                        current_config['embedding']['model'] = model_name
                except Exception as model_list_error:
                    logger.warning(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨é…ç½®çš„æ¨¡å‹åç§°: {model_list_error}")
                    model_name = current_config['embedding']['model']
                
                # æµ‹è¯•åµŒå…¥åŠŸèƒ½
                test_embedding = get_embedding("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºéªŒè¯åµŒå…¥æ¨¡å‹åŠŸèƒ½")
                
                return {
                    "success": True,
                    "message": "åµŒå…¥æ¨¡å‹APIæµ‹è¯•æˆåŠŸ",
                    "model_type": current_config["embedding"]["type"],
                    "model_name": model_name,
                    "dimension": len(test_embedding),
                    "sample_values": test_embedding[:5] if len(test_embedding) > 5 else test_embedding
                }
                    
            except Exception as api_error:
                return {
                    "success": False,
                    "message": f"åµŒå…¥æ¨¡å‹APIæµ‹è¯•å¤±è´¥: {str(api_error)}"
                }
        else:
            # SentenceTransformersæœ¬åœ°æ¨¡å‹
            return {
                "success": True,
                "message": "åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰",
                "model_type": "sentence-transformers",
                "dimension": 384,
                "sample_values": [0.1, 0.2, 0.3, 0.4, 0.5]
            }
    except Exception as e:
        return {
            "success": False,
            "message": f"åµŒå…¥æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}"
        }

@app.post("/config/update")
async def update_configuration(
    llm_type: str = Form("local"),
    llm_url: str = Form("http://192.168.22.191:8000/v1"),
    llm_model: str = Form("/home/aiteam/.cache/modelscope/hub/models/google/medgemma-27b-text-it/"),
    llm_key: str = Form(""),
    llm_temperature: float = Form(0.3),
    embed_type: str = Form("sentence-transformers"),
    embed_url: str = Form("http://192.168.22.191:8000/v1"),
    embed_key: str = Form(""),
    embed_model: str = Form("all-MiniLM-L6-v2"),
    embed_dimension: int = Form(384)
):
    """å®æ—¶æ›´æ–°ç³»ç»Ÿé…ç½®"""
    try:
        # æ›´æ–°å…¨å±€é…ç½®
        current_config["llm"] = {
            "type": llm_type,
            "url": llm_url,
            "model": llm_model,
            "key": llm_key or "local-api-key",
            "temperature": llm_temperature
        }
        
        current_config["embedding"] = {
            "type": embed_type,
            "url": embed_url,
            "key": embed_key,
            "model": embed_model,
            "dimension": embed_dimension
        }
        
        return {
            "success": True,
            "message": "é…ç½®æ›´æ–°æˆåŠŸ",
            "config": current_config
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}")

@app.get("/config/current")
async def get_current_config():
    """è·å–å½“å‰é…ç½®"""
    return {
        "success": True,
        "config": current_config
    }

@app.post("/generate")
async def generate_protocol(request: ProtocolGenerationRequest):
    """ç”Ÿæˆä¸´åºŠè¯•éªŒæ–¹æ¡ˆ - å¢å¼ºç‰ˆ"""
    try:
        # 1. è§£æç”¨æˆ·éœ€æ±‚
        requirement_prompt = f"""
        åˆ†æä»¥ä¸‹ä¸´åºŠè¯•éªŒéœ€æ±‚ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š
        {request.user_requirement}
        
        è¯·æå–ï¼šè¯ç‰©ç±»å‹ã€é€‚åº”ç—‡ã€è¯•éªŒåˆ†æœŸã€ä¸»è¦ç›®çš„ã€ç›®æ ‡äººç¾¤ç­‰ã€‚
        è¿”å›JSONæ ¼å¼ã€‚
        """
        
        extracted_info = call_local_llm(requirement_prompt, 0.1)
        
        # 2. çŸ¥è¯†åº“æ£€ç´¢
        # åŸºäºæå–çš„ä¿¡æ¯è¿›è¡Œå¤šç»´åº¦æ£€ç´¢
        search_terms = [
            request.user_requirement,
            extracted_info.get('drug_type', ''),
            extracted_info.get('indication', ''),
            f"{extracted_info.get('trial_phase', '')} ä¸´åºŠè¯•éªŒ"
        ]
        
        all_relevant_docs = []
        for term in search_terms:
            if term:
                results = await search_knowledge_embedding(term, top_k=5)
                if results['success']:
                    all_relevant_docs.extend(results['results'])
        
        # å»é‡å’Œæ’åº
        seen = set()
        unique_docs = []
        for doc in sorted(all_relevant_docs, key=lambda x: x['score'], reverse=True):
            doc_id = doc.get('content', '')[:100]  # ç”¨å‰100å­—ç¬¦ä½œä¸ºID
            if doc_id not in seen:
                seen.add(doc_id)
                unique_docs.append(doc)
        
        # 3. ç”Ÿæˆå„æ¨¡å—å†…å®¹
        protocol_sections = {}
        for module in ['ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„', 'ç ”ç©¶è®¾è®¡', 'ç ”ç©¶äººç¾¤', 'ç»™è¯æ–¹æ¡ˆ', 
                       'å®‰å…¨æ€§è¯„ä¼°', 'ç–—æ•ˆè¯„ä¼°', 'ç»Ÿè®¡åˆ†æ', 'æ•°æ®ç®¡ç†ä¸è´¨é‡æ§åˆ¶']:
            
            # è·å–è¯¥æ¨¡å—æœ€ç›¸å…³çš„çŸ¥è¯†
            module_knowledge = [doc for doc in unique_docs 
                              if any(keyword in doc['content'] 
                                    for keyword in module.split())][:3]
            
            # ç”Ÿæˆè¯¥æ¨¡å—
            module_prompt = get_module_generation_prompt(
                module, extracted_info, module_knowledge
            )
            
            module_content = call_local_llm(module_prompt, request.temperature)
            protocol_sections[module] = module_content
        
        # 4. è´¨é‡æ£€æŸ¥
        if request.include_quality_check:
            quality_check_prompt = f"""
            è¯„ä¼°ä¸´åºŠè¯•éªŒæ–¹æ¡ˆè´¨é‡ï¼š
            1. å®Œæ•´æ€§ï¼ˆ0-25åˆ†ï¼‰ï¼šæ‰€æœ‰å¿…éœ€ç« èŠ‚æ˜¯å¦é½å…¨
            2. ç§‘å­¦æ€§ï¼ˆ0-25åˆ†ï¼‰ï¼šè®¾è®¡æ˜¯å¦åˆç†ã€è®ºè¿°æ˜¯å¦ä¸¥è°¨
            3. åˆè§„æ€§ï¼ˆ0-25åˆ†ï¼‰ï¼šæ˜¯å¦ç¬¦åˆæ³•è§„è¦æ±‚
            4. ä¸€è‡´æ€§ï¼ˆ0-25åˆ†ï¼‰ï¼šå‰åé€»è¾‘æ˜¯å¦ä¸€è‡´
            
            æ–¹æ¡ˆæ¦‚è¦ï¼š{str(protocol_sections)[:2000]}
            
            ç»™å‡ºæ€»åˆ†å’Œå…·ä½“é—®é¢˜ã€‚
            """
            
            quality_result = call_local_llm(quality_check_prompt, 0.1)
        
        return {
            "success": True,
            "protocol_content": protocol_sections,
            "extracted_info": extracted_info,
            "knowledge_used": len(unique_docs),
            "quality_report": quality_result if request.include_quality_check else None
        }
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# åœ¨åç«¯æ·»åŠ æ¨¡æ¿ç«¯ç‚¹
@app.get("/templates/clinical_trial/{phase}")
async def get_clinical_trial_template(phase: str):
    """è·å–ç‰¹å®šæœŸåˆ«çš„ä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ¨¡æ¿"""
    templates = {
        "phase1": {
            "title": "IæœŸä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ¨¡æ¿",
            "sections": [
                {"title": "ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„", "required": True},
                {"title": "ç ”ç©¶è®¾è®¡", "required": True},
                {"title": "ç ”ç©¶äººç¾¤", "required": True},
                {"title": "ç ”ç©¶è¯ç‰©åŠç»™è¯æ–¹æ¡ˆ", "required": True},
                {"title": "å®‰å…¨æ€§è¯„ä¼°", "required": True, "focus": "primary"},
                {"title": "è¯ä»£åŠ¨åŠ›å­¦è¯„ä¼°", "required": True},
                {"title": "ç–—æ•ˆè¯„ä¼°", "required": False},
                {"title": "ç»Ÿè®¡åˆ†æ", "required": True},
                {"title": "æ•°æ®ç®¡ç†", "required": True},
                {"title": "ä¼¦ç†è¦æ±‚", "required": True}
            ],
            "key_points": [
                "å‰‚é‡é€’å¢è®¾è®¡ï¼ˆ3+3, BOINç­‰ï¼‰",
                "DLTå®šä¹‰å’Œè¯„ä¼°",
                "MTD/RP2Dç¡®å®š",
                "PKé‡‡æ ·æ–¹æ¡ˆ",
                "å®‰å…¨æ€§ç›‘æµ‹è®¡åˆ’"
            ]
        },
        "phase2": {
            "title": "IIæœŸä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ¨¡æ¿",
            "sections": [
                {"title": "ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„", "required": True},
                {"title": "ç ”ç©¶è®¾è®¡", "required": True},
                {"title": "ç ”ç©¶äººç¾¤", "required": True},
                {"title": "ç ”ç©¶è¯ç‰©åŠç»™è¯æ–¹æ¡ˆ", "required": True},
                {"title": "ç–—æ•ˆè¯„ä¼°", "required": True, "focus": "primary"},
                {"title": "å®‰å…¨æ€§è¯„ä¼°", "required": True},
                {"title": "ç»Ÿè®¡åˆ†æ", "required": True},
                {"title": "æ•°æ®ç®¡ç†", "required": True}
            ],
            "key_points": [
                "Simonä¸¤é˜¶æ®µè®¾è®¡",
                "ä¸»è¦ç–—æ•ˆç»ˆç‚¹ï¼ˆORRï¼‰",
                "æ¬¡è¦ç»ˆç‚¹ï¼ˆPFSã€OSã€DORï¼‰",
                "ç–—æ•ˆè¯„ä»·æ ‡å‡†ï¼ˆRECIST 1.1ï¼‰"
            ]
        }
    }
    
    template = templates.get(f"phase{phase.lower()}", None)
    if not template:
        raise HTTPException(status_code=404, detail="æ¨¡æ¿æœªæ‰¾åˆ°")
    
    return template


# ç®€åŒ–ç‰ˆæœ¬ç”Ÿæˆå™¨ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
async def generate_protocol_simplified(request: ProtocolGenerationRequest):
    """ç®€åŒ–ç‰ˆæœ¬çš„åè®®ç”Ÿæˆå™¨"""
    try:
        # è¿™é‡Œä½¿ç”¨ä½ ä¹‹å‰çš„ç®€åŒ–é€»è¾‘
        extracted_info = {
            "drug_type": "ç ”ç©¶è¯ç‰©",
            "disease": "ç›®æ ‡ç–¾ç—…",
            "phase": "I",
            "primary_objective": "è¯„ä¼°å®‰å…¨æ€§å’Œè€å—æ€§"
        }
        
        protocol_content = {
            "è¯•éªŒèƒŒæ™¯": "åŸºäºç”¨æˆ·éœ€æ±‚ç”Ÿæˆçš„è¯•éªŒèƒŒæ™¯...",
            "è¯•éªŒè®¾è®¡": "åŸºäºç”¨æˆ·éœ€æ±‚ç”Ÿæˆçš„è¯•éªŒè®¾è®¡...",
            "å—è¯•è€…é€‰æ‹©": "åŸºäºç”¨æˆ·éœ€æ±‚ç”Ÿæˆçš„å—è¯•è€…é€‰æ‹©æ ‡å‡†..."
        }
        
        quality_report = {
            "overall_score": 75.0,
            "module_scores": {"åŸºç¡€è¯„ä¼°": 75},
            "issues": ["ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ç”Ÿæˆå™¨"],
            "recommendations": ["å»ºè®®å®‰è£…å®Œæ•´ç‰ˆæœ¬ç”Ÿæˆå™¨"]
        }
        
        return {
            "success": True,
            "generated_at": datetime.now().isoformat(),
            "user_requirement": request.user_requirement,
            "extracted_info": extracted_info,
            "protocol_content": protocol_content,
            "quality_report": quality_report,
            "generation_stats": {
                "generator_type": "Simplified",
                "total_modules": len(protocol_content)
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç®€åŒ–ç”Ÿæˆå¤±è´¥: {str(e)}")

# é€‚é…å™¨å‡½æ•°ï¼šå°†embeddingæœç´¢æ¥å£é€‚é…ç»™ç”Ÿæˆå™¨
async def search_knowledge_embedding(query: str, top_k: int = 5):
    """é€‚é…å™¨å‡½æ•°ï¼šä¸ºç”Ÿæˆå™¨æä¾›embeddingæœç´¢åŠŸèƒ½"""
    try:
        if not embedded_documents:
            return {"success": True, "results": []}
        
        # è·å–æŸ¥è¯¢æ–‡æœ¬çš„å‘é‡
        query_embedding = get_embedding(query)
        
        # è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        results = []
        for doc in embedded_documents:
            similarity = cosine_similarity(query_embedding, doc['embedding'])
            
            if similarity > 0.1:  # åªè¿”å›ç›¸ä¼¼åº¦å¤§äº0.1çš„ç»“æœ
                results.append({
                    "knowledge_type": doc["knowledge_type"],
                    "content": doc["content"],
                    "metadata": doc["metadata"],
                    "score": similarity
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top_kç»“æœ
        results.sort(key=lambda x: x['score'], reverse=True)
        
        return {
            "success": True, 
            "results": results[:top_k]
        }
        
    except Exception as e:
        logger.error(f"å‘é‡æœç´¢é€‚é…å™¨å¤±è´¥: {e}")
        return {"success": False, "results": []}
    

@app.get("/knowledge/stats")
async def get_knowledge_stats():
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŸºäºçœŸå®å‘é‡æ•°æ®ï¼‰"""
    try:
        # åŠ¨æ€è®¡ç®—å„ç±»å‹çš„çœŸå®æ–‡æ¡£æ•°é‡
        real_stats = {}
        
        # ä»å‘é‡æ•°æ®åº“ä¸­ç»Ÿè®¡å„ç±»å‹æ–‡æ¡£æ•°é‡
        for knowledge_type in knowledge_stats.keys():
            count = sum(1 for doc in embedded_documents if doc["knowledge_type"] == knowledge_type)
            real_stats[knowledge_type] = {"document_count": count}
        
        # è®¡ç®—ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£çš„æ•°é‡
        user_uploaded_count = sum(1 for doc in embedded_documents if doc["knowledge_type"] == "ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£")
        real_stats["ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£"] = {"document_count": user_uploaded_count}
        
        # æ·»åŠ æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
        total_embedded = len(embedded_documents)
        total_files = len(uploaded_files)
        
        return {
            "success": True, 
            "stats": real_stats,
            "summary": {
                "total_embedded_documents": total_embedded,
                "total_uploaded_files": total_files,
                "embedding_model": current_config["embedding"]["type"],
                "avg_docs_per_file": total_embedded / total_files if total_files > 0 else 0
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.get("/knowledge/search")
async def search_knowledge(query: str, top_k: int = 5):
    """æœç´¢çŸ¥è¯†åº“ï¼ˆçœŸæ­£çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼‰"""
    try:
        if not embedded_documents:
            return {"success": True, "results": [], "message": "çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£"}
        
        # è·å–æŸ¥è¯¢æ–‡æœ¬çš„å‘é‡
        query_embedding = get_embedding(query)
        
        # è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        results = []
        for doc in embedded_documents:
            similarity = cosine_similarity(query_embedding, doc['embedding'])
            
            if similarity > 0.1:  # åªè¿”å›ç›¸ä¼¼åº¦å¤§äº0.1çš„ç»“æœ
                results.append({
                    "knowledge_type": doc["knowledge_type"],
                    "content": doc["content"][:200] + "..." if len(doc["content"]) > 200 else doc["content"],
                    "metadata": doc["metadata"],
                    "score": similarity
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›top_kç»“æœ
        results.sort(key=lambda x: x['score'], reverse=True)
        
        return {
            "success": True, 
            "results": results[:top_k],
            "search_info": {
                "query": query,
                "total_docs_searched": len(embedded_documents),
                "results_found": len(results),
                "embedding_dimension": len(query_embedding)
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"å‘é‡æœç´¢å¤±è´¥: {str(e)}")

# æ·»åŠ æ–‡æœ¬åˆ†å—åŠŸèƒ½
def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºé‡å çš„å—"""
    if len(text) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å·å¤„åˆ†å‰²
        if end < len(text):
            # åœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·å¤„å¯»æ‰¾è‡ªç„¶åˆ†å‰²ç‚¹
            for i in range(end, max(start + chunk_size // 2, start + 1), -1):
                if text[i-1] in 'ã€‚ï¼ï¼Ÿ.!?':
                    end = i
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # è®¾ç½®ä¸‹ä¸€ä¸ªå—çš„å¼€å§‹ä½ç½®ï¼ˆæœ‰é‡å ï¼‰
        start = end - overlap
        if start >= len(text):
            break
    
    return chunks

# ç®€å•çš„æ–‡æœ¬æå–åŠŸèƒ½
def extract_text_from_file(file_content: bytes, filename: str) -> List[str]:
    """ä»æ–‡ä»¶å†…å®¹ä¸­æå–æ–‡æœ¬"""
    file_extension = Path(filename).suffix.lower()
    
    try:
        if file_extension in ['.txt', '.md']:
            # æ–‡æœ¬æ–‡ä»¶
            text = file_content.decode('utf-8')
            paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
            return paragraphs if paragraphs else [text]
        
        elif file_extension == '.csv':
            # CSVæ–‡ä»¶
            import io
            import csv
            text_data = file_content.decode('utf-8')
            reader = csv.reader(io.StringIO(text_data))
            rows = []
            for row in reader:
                if any(cell.strip() for cell in row):  # è·³è¿‡ç©ºè¡Œ
                    rows.append(' | '.join(row))
            return rows
        elif file_extension == '.pdf':
            # PDFæ–‡ä»¶
            try:
                import io
                import PyPDF2
                
                pdf_file = io.BytesIO(file_content)
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                pages_text = []
                for page_num, page in enumerate(pdf_reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text.strip():
                            # æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬
                            cleaned_text = page_text.replace('\n', ' ').replace('\r', ' ')
                            # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
                            cleaned_text = ' '.join(cleaned_text.split())
                            
                            # æ›´å¼ºçš„PDFä¹±ç æ¸…ç†ï¼šç§»é™¤å¸¸è§çš„PDFæå–é—®é¢˜å­—ç¬¦
                            import re
                            # ç§»é™¤éæ‰“å°å­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡å­—ç¬¦
                            cleaned_text = re.sub(r'[^\u4e00-\u9fff\u3400-\u4dbf\w\s\.,;:!?\'"()\-\[\]{}@#$%^&*+=<>/\\|`~Â·ã€‚ï¼Œã€ï¼›ï¼šï¼ï¼Ÿ""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]+', '', cleaned_text)
                            # ç§»é™¤è¿‡å¤šçš„ç©ºæ ¼
                            cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
                            # ç§»é™¤é‡å¤çš„æ ‡ç‚¹ç¬¦å·
                            cleaned_text = re.sub(r'([.ã€‚,ï¼Œ;ï¼›:ï¼š!ï¼?ï¼Ÿ])\1+', r'\1', cleaned_text)
                            
                            if cleaned_text and len(cleaned_text.strip()) > 20:  # å¢åŠ åˆ°è‡³å°‘20ä¸ªæœ‰æ•ˆå­—ç¬¦
                                # é™åˆ¶æ¯é¡µå†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿ï¼ˆå¢åŠ åˆ°2000å­—ç¬¦ï¼‰
                                if len(cleaned_text) > 2000:
                                    cleaned_text = cleaned_text[:2000] + "..."
                                pages_text.append(f"ç¬¬{page_num+1}é¡µ: {cleaned_text}")
                            else:
                                # å¦‚æœæ¸…ç†åå†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯è¡¨æ ¼æˆ–å›¾ç‰‡é¡µï¼Œä¿ç•™åŸå§‹æ–‡æœ¬çš„ä¸€éƒ¨åˆ†
                                raw_text = page_text.strip()
                                if raw_text and len(raw_text) > 10:
                                    pages_text.append(f"ç¬¬{page_num+1}é¡µ: [å¯èƒ½åŒ…å«è¡¨æ ¼æˆ–å›¾ç‰‡] {raw_text[:100]}...")
                    except Exception as e:
                        pages_text.append(f"ç¬¬{page_num+1}é¡µè§£æé”™è¯¯: {str(e)}")
                
                if not pages_text:
                    return [f"PDFæ–‡ä»¶ {filename} æ— æ³•æå–æ–‡æœ¬å†…å®¹ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆPDFã€åŠ å¯†æ–‡ä»¶æˆ–çº¯å›¾ç‰‡æ–‡æ¡£"]
                
                return pages_text
                
            except ImportError:
                return [f"PDFè§£æéœ€è¦å®‰è£…PyPDF2åº“: pip install PyPDF2"]
            except Exception as e:
                return [f"PDFæ–‡ä»¶è§£æå¤±è´¥: {str(e)}"]
        
        elif file_extension in ['.xlsx', '.xls']:
            # Excelæ–‡ä»¶
            try:
                import io
                import pandas as pd
                
                df = pd.read_excel(io.BytesIO(file_content))
                rows = []
                # å¤„ç†è¡¨å¤´
                headers = ' | '.join(str(col) for col in df.columns)
                rows.append(f"è¡¨å¤´: {headers}")
                
                # å¤„ç†æ•°æ®è¡Œ
                for idx, row in df.iterrows():
                    row_text = ' | '.join(str(val) for val in row.values if pd.notna(val))
                    if row_text.strip():
                        rows.append(f"è¡Œ{idx+1}: {row_text}")
                
                return rows if rows else [f"Excelæ–‡ä»¶ {filename} ä¸ºç©º"]
                
            except Exception as e:
                return [f"Excelæ–‡ä»¶è§£æå¤±è´¥: {str(e)}"]
        
        elif file_extension == '.docx':
            # Wordæ–‡æ¡£
            try:
                import io
                from docx import Document
                
                doc = Document(io.BytesIO(file_content))
                paragraphs = []
                
                for para in doc.paragraphs:
                    text = para.text.strip()
                    if text:
                        paragraphs.append(text)
                
                return paragraphs if paragraphs else [f"Wordæ–‡æ¡£ {filename} æ— æ–‡æœ¬å†…å®¹"]
                
            except ImportError:
                return [f"Wordæ–‡æ¡£è§£æéœ€è¦å®‰è£…python-docxåº“: pip install python-docx"]
            except Exception as e:
                return [f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}"]
        
        else:
            # å°è¯•ä½œä¸ºæ–‡æœ¬å¤„ç†
            try:
                text = file_content.decode('utf-8')
                return [text] if text.strip() else [f"æ–‡ä»¶ {filename} å†…å®¹ä¸ºç©º"]
            except UnicodeDecodeError:
                return [f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ï¼Œæ— æ³•è§£æä¸ºæ–‡æœ¬"]
            
    except Exception as e:
        return [f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}"]

@app.post("/knowledge/upload")
async def upload_knowledge_file(
    file: UploadFile = File(...),
    knowledge_type: str = Form("ç”¨æˆ·ä¸Šä¼ æ–‡æ¡£"),
    title: Optional[str] = Form(None)
):
    """ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“ï¼ˆçœŸæ­£çš„å‘é‡åŒ–å¤„ç†ï¼‰"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = await file.read()
        
        # ä¿å­˜æ–‡ä»¶åˆ°uploadsç›®å½•
        file_path = UPLOAD_DIR / file.filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
        if file_path.exists():
            stem = file_path.stem
            suffix = file_path.suffix
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = UPLOAD_DIR / f"{stem}_{timestamp}{suffix}"
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as buffer:
            buffer.write(file_content)
        
        # æå–æ–‡æœ¬å†…å®¹
        text_chunks = extract_text_from_file(file_content, file.filename)
        
        # è¿›è¡Œæ–‡æœ¬åˆ†å—å¤„ç†
        processed_chunks = []
        for chunk in text_chunks:
            # å¯¹è¾ƒé•¿çš„æ–‡æœ¬è¿›è¡Œè¿›ä¸€æ­¥åˆ†å—
            sub_chunks = chunk_text(chunk, chunk_size=500, overlap=50)
            processed_chunks.extend(sub_chunks)
        
        # çœŸæ­£çš„å‘é‡åŒ–å¤„ç† - æ¯ä¸ªæ–‡æœ¬å—éƒ½ç”Ÿæˆembedding
        embedded_count = 0
        embeddings_info = []
        
        for i, chunk in enumerate(processed_chunks):
            try:
                # è°ƒç”¨embedding API
                embedding = get_embedding(chunk)
                
                # åˆ›å»ºæ–‡æ¡£æ¡ç›®
                doc_entry = {
                    "id": f"{file.filename}_{i}_{datetime.now().timestamp()}",
                    "content": chunk,
                    "embedding": embedding,
                    "knowledge_type": knowledge_type,
                    "metadata": {
                        "title": title or file.filename,
                        "source_file": file.filename,
                        "chunk_index": i,
                        "upload_time": datetime.now().isoformat(),
                        "file_type": file_path.suffix,
                        "embedding_dimension": len(embedding)
                    }
                }
                
                # æ·»åŠ åˆ°å…¨å±€å‘é‡æ•°æ®åº“
                embedded_documents.append(doc_entry)
                embedded_count += 1
                
                # è®°å½•embeddingä¿¡æ¯ç”¨äºè°ƒè¯•
                embeddings_info.append({
                    "chunk_length": len(chunk),
                    "embedding_dimension": len(embedding),
                    "embedding_sample": embedding[:3] if len(embedding) > 3 else embedding
                })
                
            except Exception as e:
                logger.error(f"ä¸ºæ–‡æœ¬å— {i} ç”Ÿæˆembeddingå¤±è´¥: {e}")
                continue
        
        # è®°å½•æ–‡ä»¶ä¿¡æ¯ï¼ˆåªå­˜å‚¨é¢„è§ˆï¼Œä¸å­˜å‚¨æ‰€æœ‰chunksï¼‰
        file_info = {
            "filename": file_path.name,
            "original_name": file.filename,
            "size": file_path.stat().st_size,
            "modified": file_path.stat().st_mtime,
            "knowledge_type": knowledge_type,
            "title": title or file.filename,
            "upload_time": datetime.now().isoformat(),
            "chunks_count": len(processed_chunks),
            "embedded_count": embedded_count,
            "chunks": processed_chunks[:3] if len(processed_chunks) > 3 else processed_chunks  # åªä¿å­˜å‰3ä¸ªå—ä½œä¸ºé¢„è§ˆ
        }
        
        uploaded_files.append(file_info)
        
        return {
            "success": True,
            "message": f"æ–‡ä»¶ {file.filename} ä¸Šä¼ å¹¶å‘é‡åŒ–æˆåŠŸ",
            "file_path": str(file_path),
            "records_added": embedded_count,
            "chunks_count": len(processed_chunks),
            "processing_info": {
                "file_type": file_path.suffix,
                "text_extracted": len(text_chunks) > 0,
                "chunking_applied": True,
                "embedding_applied": embedded_count > 0,
                "embedding_model": current_config["embedding"]["type"],
                "embedding_failures": len(processed_chunks) - embedded_count
            },
            "embeddings_sample": embeddings_info[:3]  # è¿”å›å‰3ä¸ªembeddingçš„ä¿¡æ¯
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@app.get("/knowledge/files")
async def list_uploaded_files():
    """åˆ—å‡ºå·²ä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        return {
            "success": True, 
            "files": uploaded_files,
            "total_embedded_documents": len(embedded_documents)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.delete("/knowledge/file/{filename}")
async def delete_knowledge_file(filename: str):
    """åˆ é™¤çŸ¥è¯†åº“ä¸­çš„æ–‡ä»¶"""
    try:
        global uploaded_files, embedded_documents
        
        # æŸ¥æ‰¾å¹¶åˆ é™¤æ–‡ä»¶ä¿¡æ¯
        file_to_delete = None
        
        for i, file_info in enumerate(uploaded_files):
            if file_info["filename"] == filename:
                file_to_delete = uploaded_files.pop(i)
                break
        
        if not file_to_delete:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶æœªæ‰¾åˆ°")
        
        # åˆ é™¤ç›¸å…³çš„å‘é‡æ–‡æ¡£
        original_count = len(embedded_documents)
        embedded_documents = [doc for doc in embedded_documents 
                            if doc["metadata"]["source_file"] != file_to_delete["original_name"]]
        deleted_vectors = original_count - len(embedded_documents)
        
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        file_path = UPLOAD_DIR / filename
        if file_path.exists():
            file_path.unlink()
        
        return {
            "success": True,
            "deleted_count": 1,
            "deleted_vectors": deleted_vectors,
            "message": f"æ–‡ä»¶ {filename} å’Œ {deleted_vectors} ä¸ªå‘é‡è®°å½•åˆ é™¤æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/knowledge/file/{filename}/details")
async def get_file_details(filename: str):
    """è·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬å†…å®¹å’Œembeddingç»“æœ"""
    try:
        # æŸ¥æ‰¾æ–‡ä»¶ä¿¡æ¯
        file_info = None
        for f in uploaded_files:
            if f["filename"] == filename or f.get("original_name") == filename:
                file_info = f
                break
        
        if not file_info:
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ {filename} æœªæ‰¾åˆ°")
        
        # æŸ¥æ‰¾ç›¸å…³çš„embeddingæ–‡æ¡£
        related_docs = [doc for doc in embedded_documents 
                       if doc["metadata"]["source_file"] == file_info.get("original_name", filename)]
        
        if not related_docs:
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æ–‡ä»¶ {filename} çš„embeddingæ•°æ®")        # è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
        original_content = ""
        content_preview = ""
        content_truncated = False
        
        try:
            file_path = UPLOAD_DIR / file_info["filename"]
            logger.info(f"ğŸ“‚ [API] æ­£åœ¨è¯»å–åŸå§‹æ–‡ä»¶: {file_path}")
            if file_path.exists():
                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„è¯»å–æ–¹å¼
                file_extension = file_path.suffix.lower()
                
                if file_extension == '.pdf':
                    # å¯¹äºPDFæ–‡ä»¶ï¼Œä»å·²å¤„ç†çš„åˆ†å—ä¸­é‡æ„å†…å®¹ï¼Œé¿å…é‡æ–°è§£æ
                    logger.info(f"ğŸ“„ [API] PDFæ–‡ä»¶ï¼Œä»åˆ†å—é‡æ„å†…å®¹")
                    if related_docs:
                        # æŒ‰é¡µé¢é¡ºåºé‡æ„PDFå†…å®¹
                        pdf_pages = {}
                        for doc in related_docs:
                            content = doc["content"]
                            # æå–é¡µé¢ä¿¡æ¯ï¼ˆå¦‚æœå†…å®¹åŒ…å«"ç¬¬Xé¡µ:"æ ‡è¯†ï¼‰
                            if content.startswith("ç¬¬") and "é¡µ:" in content:
                                try:
                                    page_num = int(content.split("é¡µ:")[0][1:])
                                    page_content = content.split("é¡µ:", 1)[1].strip()
                                    pdf_pages[page_num] = page_content
                                except:
                                    # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨å†…å®¹
                                    pdf_pages[len(pdf_pages) + 1] = content
                            else:
                                pdf_pages[len(pdf_pages) + 1] = content
                        
                        # æŒ‰é¡µé¢é¡ºåºç»„åˆå†…å®¹
                        if pdf_pages:
                            sorted_pages = sorted(pdf_pages.items())
                            full_content = "\n\n".join([f"ç¬¬{page}é¡µ:\n{content}" for page, content in sorted_pages])
                        else:
                            full_content = "\n\n".join([doc["content"] for doc in related_docs])
                    else:
                        full_content = "PDFæ–‡ä»¶æš‚æ— å¯æ˜¾ç¤ºçš„æ–‡æœ¬å†…å®¹"
                        
                else:
                    # å¯¹äºéPDFæ–‡ä»¶ï¼Œä½¿ç”¨æ™ºèƒ½ç¼–ç æ£€æµ‹
                    full_content = read_file_with_encoding_detection(file_path)
                
                # é™åˆ¶å†…å®¹é•¿åº¦ä»¥é¿å…å‰ç«¯æ˜¾ç¤ºé—®é¢˜
                MAX_CONTENT_LENGTH = 8000  # å¢åŠ åˆ°8000å­—ç¬¦ä»¥é€‚åº”PDF
                PREVIEW_LENGTH = 1500      # é¢„è§ˆ1500å­—ç¬¦
                
                if len(full_content) > MAX_CONTENT_LENGTH:
                    original_content = full_content[:MAX_CONTENT_LENGTH] + "\n\n... [å†…å®¹è¿‡é•¿å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹åŒ…å«æ›´å¤šé¡µé¢]"
                    content_truncated = True
                else:
                    original_content = full_content
                
                # ç”Ÿæˆé¢„è§ˆå†…å®¹
                if len(full_content) > PREVIEW_LENGTH:
                    content_preview = full_content[:PREVIEW_LENGTH] + "..."
                else:
                    content_preview = full_content
                    
            else:
                original_content = "åŸå§‹æ–‡ä»¶æœªæ‰¾åˆ°"
                content_preview = "åŸå§‹æ–‡ä»¶æœªæ‰¾åˆ°"
                
            logger.info(f"ğŸ“„ [API] åŸå§‹æ–‡ä»¶å†…å®¹é•¿åº¦: {len(original_content)} (æˆªæ–­: {content_truncated})")
        except Exception as e:
            logger.error(f"âŒ [API] è¯»å–åŸå§‹æ–‡ä»¶å¤±è´¥: {e}")
            original_content = f"æ— æ³•è¯»å–åŸå§‹æ–‡ä»¶: {str(e)}"
            content_preview = original_content
        
        # æ„å»ºåˆ†å—ä¿¡æ¯
        chunks = []
        for i, doc in enumerate(related_docs):
            chunk_info = {
                "id": doc["id"],
                "content": doc["content"],
                "knowledge_type": doc["knowledge_type"],
                "chunk_length": len(doc["content"]),
                "chunk_index": doc["metadata"].get("chunk_index", i),
                "embedding_dimension": len(doc["embedding"]),
                "metadata": doc["metadata"]
            }
            chunks.append(chunk_info)
        
        # æŒ‰chunk_indexæ’åº
        chunks.sort(key=lambda x: x["chunk_index"])
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        file_stats = {
            "original_size": len(original_content),
            "chunks_count": len(chunks),
            "knowledge_types": len(set(chunk["knowledge_type"] for chunk in chunks))
        }
        
        # è®¡ç®—embeddingä¿¡æ¯
        embedding_info = {
            "total_embeddings": len(chunks),
            "embedding_dimensions": chunks[0]["embedding_dimension"] if chunks else 0,
            "avg_chunk_length": sum(chunk["chunk_length"] for chunk in chunks) / len(chunks) if chunks else 0,
            "knowledge_types": list(set(chunk["knowledge_type"] for chunk in chunks))
        }
        return {
            "success": True,
            "filename": filename,
            "original_content": original_content,
            "content_preview": content_preview,
            "content_truncated": content_truncated,
            "chunks": chunks,
            "file_stats": file_stats,
            "embedding_info": embedding_info
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶è¯¦æƒ…å¤±è´¥: {str(e)}")

# =============================================================================
# æ™ºèƒ½ç”Ÿæˆå·¥ä½œæµ API ç«¯ç‚¹
# =============================================================================

# è¯·æ±‚æ¨¡å‹
class KeyInfoExtractionRequest(BaseModel):
    input_text: str

class OutlineGenerationRequest(BaseModel):
    confirmed_info: Dict[str, Any]
    original_input: str

class ProtocolStreamRequest(BaseModel):
    confirmed_info: Dict[str, Any]
    outline: List[Dict[str, Any]]
    settings: Dict[str, Any]

class ExportProtocolRequest(BaseModel):
    content: str
    format: str
    metadata: Dict[str, Any]


@app.post("/extract_key_info")
async def extract_key_info(request: KeyInfoExtractionRequest):
    """æ­¥éª¤1ï¼šä»è¾“å…¥æ–‡æœ¬ä¸­æå–ä¸´åºŠè¯•éªŒæ–¹æ¡ˆçš„å…³é”®ä¿¡æ¯"""
    try:
        # æ„å»ºä¸“é—¨é’ˆå¯¹ä¸´åºŠè¯•éªŒæ–¹æ¡ˆçš„æå–æç¤ºè¯
        extraction_prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸´åºŠè¯•éªŒæ–¹æ¡ˆä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ä¸´åºŠè¯•éªŒæ–¹æ¡ˆçš„å…³é”®ä¿¡æ¯ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚

è¾“å…¥æ–‡æœ¬ï¼š
{request.input_text}

è¯·æå–ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
1. drug_typeï¼ˆè¯ç‰©ç±»å‹ï¼‰ï¼šå¦‚TCR-Tç»†èƒã€CAR-Tç»†èƒã€å…ç–«æ£€æŸ¥ç‚¹æŠ‘åˆ¶å‰‚ç­‰
2. diseaseï¼ˆç›®æ ‡ç–¾ç—…ï¼‰ï¼šå…·ä½“çš„ç™Œç—‡ç±»å‹å’Œåˆ†æœŸï¼Œå¦‚æ™šæœŸè‚ºé³ç™Œã€å¤å‘éš¾æ²»æ€§æ·‹å·´ç˜¤ç­‰
3. trial_phaseï¼ˆè¯•éªŒåˆ†æœŸï¼‰ï¼šIæœŸã€IIæœŸã€IIIæœŸæˆ–I/IIæœŸç­‰
4. primary_objectiveï¼ˆä¸»è¦ç›®çš„ï¼‰ï¼šå®‰å…¨æ€§ã€è€å—æ€§ã€æœ‰æ•ˆæ€§ã€å‰‚é‡æ¢ç´¢ç­‰
5. primary_endpointï¼ˆä¸»è¦ç»ˆç‚¹ï¼‰ï¼šå¦‚MTDã€RP2Dã€ORRã€PFSã€OSç­‰
6. secondary_endpointsï¼ˆæ¬¡è¦ç»ˆç‚¹ï¼‰ï¼šåˆ—è¡¨å½¢å¼ï¼Œå¦‚DCRã€DORã€å®‰å…¨æ€§ç­‰
7. patient_populationï¼ˆç›®æ ‡äººç¾¤ï¼‰ï¼šè¯¦ç»†çš„æ‚£è€…ç‰¹å¾æè¿°
8. estimated_enrollmentï¼ˆé¢„è®¡å…¥ç»„ï¼‰ï¼šå…¥ç»„äººæ•°èŒƒå›´
9. study_designï¼ˆç ”ç©¶è®¾è®¡ï¼‰ï¼šå•è‡‚/åŒè‡‚ã€å¼€æ”¾/ç›²æ³•ã€å‰‚é‡é€’å¢ç­‰
10. inclusion_criteria_hintsï¼ˆå…¥ç»„æ ‡å‡†æç¤ºï¼‰ï¼šå…³é”®çš„å…¥ç»„è¦æ±‚
11. treatment_lineï¼ˆæ²»ç–—çº¿æ•°ï¼‰ï¼šä¸€çº¿ã€äºŒçº¿æˆ–å¤šçº¿æ²»ç–—
12. biomarker_requirementsï¼ˆç”Ÿç‰©æ ‡å¿—ç‰©è¦æ±‚ï¼‰ï¼šå¦‚HLAåˆ†å‹ã€æŠ—åŸè¡¨è¾¾ç­‰

æ³¨æ„äº‹é¡¹ï¼š
- å¦‚æœæŸé¡¹ä¿¡æ¯æœªæ˜ç¡®æåŠï¼Œè¯·æ ¹æ®ä¸´åºŠè¯•éªŒå¸¸è§„åšæ³•è¿›è¡Œåˆç†æ¨æµ‹
- å¯¹äºIæœŸè¯•éªŒï¼Œä¸»è¦ç»ˆç‚¹é€šå¸¸æ˜¯å®‰å…¨æ€§å’Œè€å—æ€§
- å¯¹äºç»†èƒæ²»ç–—äº§å“ï¼Œéœ€è¦ç‰¹åˆ«å…³æ³¨HLAé…å‹å’Œé¶æŠ—åŸè¡¨è¾¾
- è¿”å›çš„JSONæ ¼å¼å¿…é¡»ä¸¥æ ¼ç¬¦åˆè¦æ±‚

è¿”å›æ ¼å¼ç¤ºä¾‹ï¼š
{{
    "drug_type": "TCR-Tç»†èƒæ²»ç–—",
    "disease": "æ™šæœŸè‚ºé³ç™Œ",
    "trial_phase": "IæœŸ",
    "primary_objective": "è¯„ä¼°å®‰å…¨æ€§ã€è€å—æ€§å¹¶ç¡®å®šRP2D",
    "primary_endpoint": "å‰‚é‡é™åˆ¶æ€§æ¯’æ€§(DLT)å’Œæœ€å¤§è€å—å‰‚é‡(MTD)",
    "secondary_endpoints": ["å®¢è§‚ç¼“è§£ç‡(ORR)", "ç–¾ç—…æ§åˆ¶ç‡(DCR)", "æ— è¿›å±•ç”Ÿå­˜æœŸ(PFS)"],
    "patient_population": "æ—¢å¾€æ ‡å‡†æ²»ç–—å¤±è´¥çš„æ™šæœŸè‚ºé³ç™Œæ‚£è€…",
    "estimated_enrollment": "12-18ä¾‹",
    "study_design": "å¼€æ”¾æ ‡ç­¾ã€å•è‡‚ã€å‰‚é‡é€’å¢IæœŸç ”ç©¶",
    "inclusion_criteria_hints": "HLA-A*02:01é˜³æ€§ï¼Œè‚¿ç˜¤è¡¨è¾¾é¶æŠ—åŸ",
    "treatment_line": "äºŒçº¿åŠä»¥ä¸Š",
    "biomarker_requirements": "HLA-A*02:01é˜³æ€§ï¼ŒNY-ESO-1è¡¨è¾¾é˜³æ€§"
}}
"""
        
        # è°ƒç”¨LLMè¿›è¡Œå…³é”®ä¿¡æ¯æå–
        response = call_local_llm(extraction_prompt, temperature=0.1)
        
        try:
            # è§£æJSONå“åº”
            import re
            json_match = re.search(r'\{.*?\}', response, re.DOTALL)
            if json_match:
                extracted_info = json.loads(json_match.group())
            else:
                # å¦‚æœæ— æ³•è§£æJSONï¼Œè¿”å›é»˜è®¤çš„ä¸´åºŠè¯•éªŒç›¸å…³å­—æ®µ
                extracted_info = {
                    "drug_type": "å¾…ç¡®å®šçš„ç ”ç©¶è¯ç‰©",
                    "disease": "å¾…ç¡®å®šçš„ç›®æ ‡é€‚åº”ç—‡",
                    "trial_phase": "IæœŸ",
                    "primary_objective": "è¯„ä¼°å®‰å…¨æ€§å’Œè€å—æ€§",
                    "primary_endpoint": "DLTå’ŒMTD",
                    "secondary_endpoints": ["ORR", "DCR", "PFS"],
                    "patient_population": "å¾…ç¡®å®šçš„æ‚£è€…äººç¾¤",
                    "estimated_enrollment": "å¾…ç¡®å®š",
                    "study_design": "å¼€æ”¾æ ‡ç­¾ã€å‰‚é‡é€’å¢ç ”ç©¶",
                    "inclusion_criteria_hints": "å¾…è¡¥å……",
                    "treatment_line": "å¾…ç¡®å®š",
                    "biomarker_requirements": "å¾…ç¡®å®š"
                }
                
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
            required_fields = [
                "drug_type", "disease", "trial_phase", "primary_objective",
                "primary_endpoint", "patient_population", "estimated_enrollment",
                "study_design"
            ]
            
            for field in required_fields:
                if field not in extracted_info:
                    extracted_info[field] = "å¾…è¡¥å……"
                    
        except Exception as parse_error:
            logger.error(f"JSONè§£æå¤±è´¥: {parse_error}")
            # è¿”å›åŸºç¡€æ¨¡æ¿
            extracted_info = {
                "drug_type": "ç ”ç©¶è¯ç‰©",
                "disease": "ç›®æ ‡ç–¾ç—…",
                "trial_phase": "IæœŸ",
                "primary_objective": "å®‰å…¨æ€§å’Œè€å—æ€§è¯„ä¼°",
                "primary_endpoint": "DLT/MTD",
                "secondary_endpoints": ["ORR", "å®‰å…¨æ€§"],
                "patient_population": "ç›®æ ‡æ‚£è€…äººç¾¤",
                "estimated_enrollment": "12-30ä¾‹",
                "study_design": "å‰‚é‡é€’å¢ç ”ç©¶",
                "error": "ä¿¡æ¯æå–éƒ¨åˆ†å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¡¥å……"
            }
        
        return {
            "success": True,
            "extracted_info": extracted_info,
            "original_response": response,
            "extraction_quality": validate_extraction_quality(extracted_info)
        }
        
    except Exception as e:
        logger.error(f"æå–å…³é”®ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å…³é”®ä¿¡æ¯æå–å¤±è´¥: {str(e)}")

def validate_extraction_quality(info):
    """éªŒè¯æå–ä¿¡æ¯çš„è´¨é‡"""
    quality_score = 100
    issues = []
    
    # æ£€æŸ¥å…³é”®å­—æ®µ
    if "å¾…" in str(info.get("drug_type", "")):
        quality_score -= 20
        issues.append("è¯ç‰©ç±»å‹æœªæ˜ç¡®")
        
    if "å¾…" in str(info.get("disease", "")):
        quality_score -= 20
        issues.append("ç›®æ ‡ç–¾ç—…æœªæ˜ç¡®")
        
    if not info.get("trial_phase"):
        quality_score -= 15
        issues.append("è¯•éªŒåˆ†æœŸç¼ºå¤±")
        
    if not info.get("primary_endpoint"):
        quality_score -= 15
        issues.append("ä¸»è¦ç»ˆç‚¹ç¼ºå¤±")
        
    return {
        "score": quality_score,
        "issues": issues,
        "recommendation": "å»ºè®®è¡¥å……å®Œå–„" if quality_score < 80 else "ä¿¡æ¯å®Œæ•´"
    }


@app.post("/generate_outline")
async def generate_outline(request: OutlineGenerationRequest):
    """æ­¥éª¤2ï¼šåŸºäºç¡®è®¤ä¿¡æ¯ç”Ÿæˆåè®®å¤§çº²"""
    try:
        # æ„å»ºç¬¦åˆä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ ‡å‡†çš„å¤§çº²ç”Ÿæˆæç¤ºè¯
        outline_prompt = f"""
åŸºäºä»¥ä¸‹ç¡®è®¤çš„ä¸´åºŠè¯•éªŒä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ä¸´åºŠè¯•éªŒæ–¹æ¡ˆå¤§çº²ï¼š

ç¡®è®¤ä¿¡æ¯ï¼š
- è¯ç‰©ç±»å‹ï¼š{request.confirmed_info.get('drug_type', 'æœªæŒ‡å®š')}
- é€‚åº”ç—‡ï¼š{request.confirmed_info.get('indication', 'æœªæŒ‡å®š')}
- ç ”ç©¶é˜¶æ®µï¼š{request.confirmed_info.get('study_phase', 'æœªæŒ‡å®š')}
- ç ”ç©¶ç±»å‹ï¼š{request.confirmed_info.get('study_type', 'æœªæŒ‡å®š')}
- ä¸»è¦ç›®çš„ï¼š{request.confirmed_info.get('primary_objectives', 'æœªæŒ‡å®š')}
- ç›®æ ‡äººç¾¤ï¼š{request.confirmed_info.get('patient_population', 'æœªæŒ‡å®š')}
- ä¸»è¦ç»ˆç‚¹ï¼š{request.confirmed_info.get('primary_endpoint', 'æœªæŒ‡å®š')}
- é¢„è®¡å…¥ç»„ï¼š{request.confirmed_info.get('estimated_enrollment', 'æœªæŒ‡å®š')}

åŸå§‹éœ€æ±‚ï¼š
{request.original_input}

è¯·ç”Ÿæˆç¬¦åˆICH-GCPæ ‡å‡†çš„ä¸´åºŠè¯•éªŒæ–¹æ¡ˆå¤§çº²ï¼ŒåŒ…å«ä»¥ä¸‹æ ‡å‡†ç« èŠ‚ï¼ˆè¿”å›JSONæ•°ç»„æ ¼å¼ï¼‰ï¼š

[
    {{
        "title": "1. ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„",
        "content": "åŒ…æ‹¬ç–¾ç—…èƒŒæ™¯ã€è¯ç‰©æœºåˆ¶ã€ç ”ç©¶ç†è®ºåŸºç¡€ã€ä¸»è¦ç›®çš„å’Œæ¬¡è¦ç›®çš„",
        "subsections": ["1.1 ç–¾ç—…èƒŒæ™¯", "1.2 è¯ç‰©ä»‹ç»", "1.3 ç ”ç©¶ç†è®ºåŸºç¡€", "1.4 ç ”ç©¶ç›®çš„"]
    }},
    {{
        "title": "2. ç ”ç©¶è®¾è®¡",
        "content": "è¯•éªŒç±»å‹ã€åˆ†ç»„è®¾è®¡ã€éšæœºåŒ–ã€ç›²æ³•ã€å¯¹ç…§é€‰æ‹©ç­‰",
        "subsections": ["2.1 è¯•éªŒç±»å‹", "2.2 ç ”ç©¶ç»ˆç‚¹", "2.3 è¯•éªŒè®¾è®¡å›¾"]
    }},
    {{
        "title": "3. ç ”ç©¶äººç¾¤",
        "content": "å…¥é€‰æ ‡å‡†ã€æ’é™¤æ ‡å‡†ã€é€€å‡ºæ ‡å‡†ã€ä¸­æ­¢æ ‡å‡†",
        "subsections": ["3.1 å…¥é€‰æ ‡å‡†", "3.2 æ’é™¤æ ‡å‡†", "3.3 é€€å‡ºæ ‡å‡†", "3.4 ä¸­æ­¢æ ‡å‡†"]
    }},
    {{
        "title": "4. ç ”ç©¶è¯ç‰©åŠç»™è¯æ–¹æ¡ˆ",
        "content": "è¯•éªŒè¯ç‰©ã€å¯¹ç…§è¯ç‰©ã€ç»™è¯æ–¹æ¡ˆã€å‰‚é‡è°ƒæ•´ã€åˆå¹¶ç”¨è¯",
        "subsections": ["4.1 è¯•éªŒè¯ç‰©", "4.2 ç»™è¯æ–¹æ¡ˆ", "4.3 å‰‚é‡è°ƒæ•´", "4.4 åˆå¹¶ç”¨è¯"]
    }},
    {{
        "title": "5. ç ”ç©¶æµç¨‹",
        "content": "ç­›é€‰æœŸã€æ²»ç–—æœŸã€éšè®¿æœŸçš„è®¿è§†å®‰æ’å’Œæ£€æŸ¥é¡¹ç›®",
        "subsections": ["5.1 ç ”ç©¶æµç¨‹å›¾", "5.2 ç­›é€‰æœŸ", "5.3 æ²»ç–—æœŸ", "5.4 éšè®¿æœŸ"]
    }},
    {{
        "title": "6. å®‰å…¨æ€§è¯„ä¼°",
        "content": "ä¸è‰¯äº‹ä»¶å®šä¹‰ã€ä¸¥é‡ç¨‹åº¦åˆ†çº§ã€å› æœå…³ç³»åˆ¤å®šã€å®‰å…¨æ€§ç›‘æµ‹",
        "subsections": ["6.1 å®‰å…¨æ€§å‚æ•°", "6.2 ä¸è‰¯äº‹ä»¶", "6.3 ä¸¥é‡ä¸è‰¯äº‹ä»¶", "6.4 å®‰å…¨æ€§ç›‘æµ‹"]
    }},
    {{
        "title": "7. ç–—æ•ˆè¯„ä¼°",
        "content": "ç–—æ•ˆè¯„ä»·æ ‡å‡†ã€è¯„ä¼°æ—¶é—´ç‚¹ã€ç–—æ•ˆæŒ‡æ ‡å®šä¹‰",
        "subsections": ["7.1 ç–—æ•ˆè¯„ä»·æ ‡å‡†", "7.2 ç–—æ•ˆè¯„ä¼°æ—¶é—´", "7.3 ç–—æ•ˆæŒ‡æ ‡å®šä¹‰"]
    }},
    {{
        "title": "8. ç»Ÿè®¡åˆ†æ",
        "content": "æ ·æœ¬é‡è®¡ç®—ã€ç»Ÿè®¡åˆ†æé›†ã€åˆ†ææ–¹æ³•ã€äºšç»„åˆ†æ",
        "subsections": ["8.1 æ ·æœ¬é‡è®¡ç®—", "8.2 åˆ†ææ•°æ®é›†", "8.3 ç»Ÿè®¡æ–¹æ³•", "8.4 æœŸä¸­åˆ†æ"]
    }},
    {{
        "title": "9. æ•°æ®ç®¡ç†ä¸è´¨é‡æ§åˆ¶",
        "content": "æ•°æ®é‡‡é›†ã€è´¨é‡ä¿è¯ã€ç›‘æŸ¥è®¡åˆ’ã€æ•°æ®ç®¡ç†",
        "subsections": ["9.1 æ•°æ®ç®¡ç†", "9.2 è´¨é‡ä¿è¯", "9.3 ç›‘æŸ¥è®¡åˆ’"]
    }},
    {{
        "title": "10. ä¼¦ç†ä¸æ³•è§„",
        "content": "ä¼¦ç†å®¡æŸ¥ã€çŸ¥æƒ…åŒæ„ã€å—è¯•è€…ä¿æŠ¤ã€æ³•è§„è¦æ±‚",
        "subsections": ["10.1 ä¼¦ç†è¦æ±‚", "10.2 çŸ¥æƒ…åŒæ„", "10.3 æ•°æ®ä¿å¯†", "10.4 æ³•è§„ç¬¦åˆæ€§"]
    }}
]

è¯·ç¡®ä¿æ¯ä¸ªç« èŠ‚éƒ½åŒ…å«é€‚å½“çš„å­ç« èŠ‚(subsections)ï¼Œå¹¶ä¸”å†…å®¹æè¿°å‡†ç¡®åæ˜ è¯¥ç« èŠ‚åº”åŒ…å«çš„è¦ç´ ã€‚
"""
        
        # è°ƒç”¨LLMç”Ÿæˆå¤§çº²
        response = call_local_llm(outline_prompt, temperature=0.2)
        
        try:
            # å°è¯•è§£æJSONå“åº”
            import re
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                outline = json.loads(json_match.group())
                # ç¡®ä¿æ¯ä¸ªç« èŠ‚éƒ½æœ‰subsectionså­—æ®µ
                for section in outline:
                    if 'subsections' not in section:
                        section['subsections'] = []
            else:
                # ä½¿ç”¨æ ‡å‡†å¤§çº²æ¨¡æ¿
                outline = get_standard_protocol_outline(request.confirmed_info)
        except:
            # JSONè§£æå¤±è´¥æ—¶ä½¿ç”¨æ ‡å‡†æ¨¡æ¿
            outline = get_standard_protocol_outline(request.confirmed_info)
        
        return {
            "success": True,
            "outline": outline,
            "original_response": response
        }
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¤§çº²å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")

def get_standard_protocol_outline(confirmed_info):
    """è·å–æ ‡å‡†çš„ä¸´åºŠè¯•éªŒæ–¹æ¡ˆå¤§çº²æ¨¡æ¿"""
    drug_type = confirmed_info.get('drug_type', 'è¯•éªŒè¯ç‰©')
    indication = confirmed_info.get('indication', 'ç›®æ ‡é€‚åº”ç—‡')
    study_phase = confirmed_info.get('study_phase', 'IæœŸ')
    
    return [
        {
            "title": "1. ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„",
            "content": f"ä»‹ç»{indication}çš„ç–¾ç—…èƒŒæ™¯ã€{drug_type}çš„ä½œç”¨æœºåˆ¶ã€ç ”ç©¶ç†è®ºåŸºç¡€å’Œç ”ç©¶ç›®çš„",
            "subsections": ["1.1 ç–¾ç—…èƒŒæ™¯", "1.2 è¯ç‰©ä»‹ç»", "1.3 ç ”ç©¶ç†è®ºåŸºç¡€", "1.4 ç ”ç©¶ç›®çš„"]
        },
        {
            "title": "2. ç ”ç©¶è®¾è®¡",
            "content": f"{study_phase}ä¸´åºŠè¯•éªŒçš„æ€»ä½“è®¾è®¡ã€ç ”ç©¶ç»ˆç‚¹å’Œè¯•éªŒæ–¹æ¡ˆæ¦‚è¿°",
            "subsections": ["2.1 è¯•éªŒç±»å‹ä¸è®¾è®¡", "2.2 ä¸»è¦ç»ˆç‚¹", "2.3 æ¬¡è¦ç»ˆç‚¹", "2.4 è¯•éªŒæµç¨‹å›¾"]
        },
        {
            "title": "3. ç ”ç©¶äººç¾¤",
            "content": "è¯¦ç»†çš„å—è¯•è€…é€‰æ‹©æ ‡å‡†ï¼ŒåŒ…æ‹¬å…¥é€‰ã€æ’é™¤ã€é€€å‡ºå’Œä¸­æ­¢æ ‡å‡†",
            "subsections": ["3.1 å…¥é€‰æ ‡å‡†", "3.2 æ’é™¤æ ‡å‡†", "3.3 é€€å‡ºæ ‡å‡†", "3.4 ä¸­æ­¢æ ‡å‡†"]
        },
        {
            "title": "4. ç ”ç©¶è¯ç‰©åŠç»™è¯æ–¹æ¡ˆ",
            "content": f"{drug_type}çš„ç»™è¯æ–¹æ¡ˆã€å‰‚é‡è®¾è®¡ã€ç»™è¯é€”å¾„å’Œç”¨è¯ç®¡ç†",
            "subsections": ["4.1 è¯•éªŒè¯ç‰©", "4.2 ç»™è¯æ–¹æ¡ˆ", "4.3 å‰‚é‡è°ƒæ•´", "4.4 åˆå¹¶ç”¨è¯ç®¡ç†"]
        },
        {
            "title": "5. ç ”ç©¶æµç¨‹ä¸è®¿è§†å®‰æ’",
            "content": "è¯¦ç»†çš„ç ”ç©¶æµç¨‹ã€è®¿è§†æ—¶é—´è¡¨å’Œå„é˜¶æ®µæ£€æŸ¥é¡¹ç›®",
            "subsections": ["5.1 ç ”ç©¶æµç¨‹æ€»è§ˆ", "5.2 ç­›é€‰æœŸ", "5.3 æ²»ç–—æœŸ", "5.4 éšè®¿æœŸ", "5.5 è®¿è§†çª—å£æœŸ"]
        },
        {
            "title": "6. å®‰å…¨æ€§è¯„ä¼°",
            "content": "å…¨é¢çš„å®‰å…¨æ€§ç›‘æµ‹è®¡åˆ’ã€ä¸è‰¯äº‹ä»¶å¤„ç†æµç¨‹",
            "subsections": ["6.1 å®‰å…¨æ€§å‚æ•°", "6.2 ä¸è‰¯äº‹ä»¶å®šä¹‰ä¸åˆ†çº§", "6.3 ä¸¥é‡ä¸è‰¯äº‹ä»¶", "6.4 å‰‚é‡é™åˆ¶æ¯’æ€§(DLT)"]
        },
        {
            "title": "7. ç–—æ•ˆè¯„ä¼°",
            "content": "ç–—æ•ˆè¯„ä»·æ ‡å‡†ã€è¯„ä¼°æ–¹æ³•å’Œæ—¶é—´ç‚¹",
            "subsections": ["7.1 ç–—æ•ˆè¯„ä»·æ ‡å‡†", "7.2 ç–—æ•ˆè¯„ä¼°æ—¶é—´ç‚¹", "7.3 ç–—æ•ˆæŒ‡æ ‡å®šä¹‰", "7.4 æ¢ç´¢æ€§ç»ˆç‚¹"]
        },
        {
            "title": "8. ç»Ÿè®¡åˆ†æè®¡åˆ’",
            "content": "è¯¦ç»†çš„ç»Ÿè®¡åˆ†ææ–¹æ³•ã€æ ·æœ¬é‡è®¡ç®—å’Œæ•°æ®å¤„ç†è®¡åˆ’",
            "subsections": ["8.1 æ ·æœ¬é‡è®¡ç®—", "8.2 åˆ†ææ•°æ®é›†", "8.3 ç»Ÿè®¡åˆ†ææ–¹æ³•", "8.4 æœŸä¸­åˆ†æ", "8.5 äºšç»„åˆ†æ"]
        },
        {
            "title": "9. æ•°æ®ç®¡ç†ä¸è´¨é‡æ§åˆ¶",
            "content": "æ•°æ®ç®¡ç†æµç¨‹ã€è´¨é‡ä¿è¯æªæ–½å’Œç›‘æŸ¥è®¡åˆ’",
            "subsections": ["9.1 æ•°æ®é‡‡é›†ä¸ç®¡ç†", "9.2 è´¨é‡ä¿è¯", "9.3 ä¸´åºŠç›‘æŸ¥", "9.4 ç¨½æŸ¥"]
        },
        {
            "title": "10. ä¼¦ç†ã€æ³•è§„ä¸ç®¡ç†",
            "content": "ä¼¦ç†å®¡æŸ¥è¦æ±‚ã€çŸ¥æƒ…åŒæ„æµç¨‹ã€å—è¯•è€…ä¿æŠ¤å’Œæ³•è§„ç¬¦åˆæ€§",
            "subsections": ["10.1 ä¼¦ç†å§”å‘˜ä¼šå®¡æŸ¥", "10.2 çŸ¥æƒ…åŒæ„", "10.3 å—è¯•è€…ä¿æŠ¤", "10.4 æ–¹æ¡ˆåç¦»å¤„ç†", "10.5 ç ”ç©¶ç»ˆæ­¢"]
        }
    ]

# ä¸´åºŠè¯•éªŒæ–¹æ¡ˆåˆ†æ¨¡å—ç”Ÿæˆæç¤ºè¯æ¨¡æ¿
def get_module_generation_prompt(module_name, confirmed_info, knowledge_context=""):
    """æ ¹æ®æ¨¡å—åç§°è¿”å›ç›¸åº”çš„ç”Ÿæˆæç¤ºè¯"""
    
    drug_type = confirmed_info.get('drug_type', 'è¯•éªŒè¯ç‰©')
    indication = confirmed_info.get('indication', confirmed_info.get('disease', 'ç›®æ ‡é€‚åº”ç—‡'))
    study_phase = confirmed_info.get('study_phase', 'IæœŸ')
    
    # é€šç”¨çš„é«˜è´¨é‡è¦æ±‚
    quality_requirements = """
    
ç”Ÿæˆè¦æ±‚ï¼š
1. å†…å®¹å¿…é¡»ä¸“ä¸šã€å‡†ç¡®ã€è¯¦å®
2. ç¬¦åˆICH-GCPå’Œä¸­å›½è¯ç›‘å±€çš„ç›¸å…³è¦æ±‚
3. å¼•ç”¨æ•°æ®å¿…é¡»æ ‡æ³¨æ¥æº
4. ä½¿ç”¨æ ‡å‡†çš„åŒ»å­¦æœ¯è¯­ï¼Œå¿…è¦æ—¶åŠ æ³¨è‹±æ–‡
5. é€»è¾‘æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
6. é¿å…ä½¿ç”¨æ¨¡ç³Šæˆ–ä¸ç¡®å®šçš„è¡¨è¿°
"""
    
    prompts = {
        "ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„": f"""
ä½œä¸ºä¸´åºŠè¯•éªŒæ–¹æ¡ˆæ’°å†™ä¸“å®¶ï¼Œè¯·æ’°å†™{drug_type}æ²»ç–—{indication}çš„{study_phase}ä¸´åºŠè¯•éªŒæ–¹æ¡ˆçš„"ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„"ç« èŠ‚ã€‚

å¿…é¡»åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

### 1.1 ç–¾ç—…èƒŒæ™¯
- {indication}çš„æµè¡Œç—…å­¦æ•°æ®ï¼ˆå‘ç—…ç‡ã€æ­»äº¡ç‡ã€åœ°åŸŸåˆ†å¸ƒï¼‰
- ç–¾ç—…çš„ç—…ç†ç”Ÿç†å­¦ç‰¹å¾å’Œåˆ†å­æœºåˆ¶
- ç›®å‰çš„æ ‡å‡†æ²»ç–—æ–¹æ¡ˆåŠå…¶å±€é™æ€§
- æœªæ»¡è¶³çš„ä¸´åºŠéœ€æ±‚

### 1.2 è¯ç‰©ä»‹ç»
- {drug_type}çš„ä½œç”¨æœºåˆ¶å’Œè¯ç†å­¦ç‰¹æ€§
- éä¸´åºŠç ”ç©¶æ•°æ®æ€»ç»“ï¼ˆè¯æ•ˆå­¦ã€è¯ä»£åŠ¨åŠ›å­¦ã€æ¯’ç†å­¦ï¼‰
- åŒç±»è¯ç‰©çš„ç ”å‘ç°çŠ¶å’Œä¸´åºŠæ•°æ®
- æœ¬è¯ç‰©çš„åˆ›æ–°ç‚¹å’Œæ½œåœ¨ä¼˜åŠ¿

### 1.3 ç ”ç©¶ç†è®ºåŸºç¡€
- é€‰æ‹©{indication}ä½œä¸ºç›®æ ‡é€‚åº”ç—‡çš„ç§‘å­¦ä¾æ®
- å‰‚é‡é€‰æ‹©çš„ç†è®ºåŸºç¡€
- ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆå¦‚é€‚ç”¨ï¼‰çš„é€‰æ‹©ä¾æ®
- é¢„æœŸçš„ä¸´åºŠè·ç›Š

### 1.4 ç ”ç©¶ç›®çš„
- ä¸»è¦ç›®çš„ï¼š{confirmed_info.get('primary_objective', 'è¯„ä¼°å®‰å…¨æ€§å’Œè€å—æ€§')}
- æ¬¡è¦ç›®çš„ï¼š{', '.join(confirmed_info.get('secondary_objectives', ['åˆæ­¥ç–—æ•ˆè¯„ä¼°', 'PK/PDç‰¹å¾']))}
- æ¢ç´¢æ€§ç›®çš„ï¼šç”Ÿç‰©æ ‡å¿—ç‰©æ¢ç´¢ã€ä½œç”¨æœºåˆ¶éªŒè¯ç­‰

å‚è€ƒçŸ¥è¯†åº“ï¼š
{knowledge_context}

{quality_requirements}
""",

        "ç ”ç©¶è®¾è®¡": f"""
è¯·è¯¦ç»†è®¾è®¡{drug_type}æ²»ç–—{indication}çš„{study_phase}ä¸´åºŠè¯•éªŒæ–¹æ¡ˆçš„"ç ”ç©¶è®¾è®¡"ç« èŠ‚ã€‚

### 2.1 è¯•éªŒæ€»ä½“è®¾è®¡
- è¯•éªŒç±»å‹ï¼š{confirmed_info.get('study_design', 'å¼€æ”¾æ ‡ç­¾ã€å•è‡‚ã€å‰‚é‡é€’å¢')}
- è¯•éªŒåˆ†æœŸï¼šå‰‚é‡é€’å¢æœŸ + å‰‚é‡æ‰©å±•æœŸï¼ˆå¦‚é€‚ç”¨ï¼‰
- é¢„è®¡è¯•éªŒå‘¨æœŸï¼šç­›é€‰æœŸï¼ˆXå‘¨ï¼‰+ æ²»ç–—æœŸï¼ˆXä¸ªå‘¨æœŸï¼‰+ éšè®¿æœŸï¼ˆXä¸ªæœˆï¼‰

### 2.2 å‰‚é‡é€’å¢è®¾è®¡ï¼ˆå¦‚é€‚ç”¨äºIæœŸï¼‰
- å‰‚é‡é€’å¢æ–¹æ¡ˆï¼š3+3è®¾è®¡ / BOINè®¾è®¡ / CRMè®¾è®¡
- èµ·å§‹å‰‚é‡ï¼šåŸºäºéä¸´åºŠæ•°æ®çš„1/10 NOAELæˆ–1/6 STD10
- å‰‚é‡é€’å¢å¹…åº¦ï¼šé¦–æ¬¡100%ï¼Œåç»­æ ¹æ®DLTæƒ…å†µè°ƒæ•´ï¼ˆ50%æˆ–33%ï¼‰
- DLTå®šä¹‰çª—å£æœŸï¼šç¬¬1ä¸ªæ²»ç–—å‘¨æœŸï¼ˆ28å¤©ï¼‰
- å‰‚é‡é™åˆ¶æ€§æ¯’æ€§(DLT)å®šä¹‰ï¼š
  * 4çº§è¡€æ¶²å­¦æ¯’æ€§æŒç»­â‰¥7å¤©
  * 3çº§éè¡€æ¶²å­¦æ¯’æ€§ï¼ˆé™¤å¤–å¯æ§åˆ¶çš„æ¶å¿ƒ/å‘•åï¼‰
  * æ²»ç–—ç›¸å…³çš„å»¶è¿Ÿ>14å¤©
  * å…¶ä»–ç ”ç©¶è€…åˆ¤å®šçš„ä¸å¯æ¥å—æ¯’æ€§

### 2.3 ç ”ç©¶ç»ˆç‚¹
ä¸»è¦ç»ˆç‚¹ï¼š
- {confirmed_info.get('primary_endpoint', 'MTDå’ŒRP2Dçš„ç¡®å®š')}
- å®‰å…¨æ€§å’Œè€å—æ€§è¯„ä¼°

æ¬¡è¦ç»ˆç‚¹ï¼š
{chr(10).join('- ' + ep for ep in confirmed_info.get('secondary_endpoints', ['ORR', 'DCR', 'DOR', 'PFS', 'OS', 'PKå‚æ•°']))}

æ¢ç´¢æ€§ç»ˆç‚¹ï¼š
- å…ç–«ç›‘æµ‹æŒ‡æ ‡
- ç”Ÿç‰©æ ‡å¿—ç‰©ä¸ç–—æ•ˆç›¸å…³æ€§
- è€è¯æœºåˆ¶æ¢ç´¢

### 2.4 æ ·æœ¬é‡è®¡ç®—
- å‰‚é‡é€’å¢æœŸï¼š{confirmed_info.get('dose_escalation_n', '18-24ä¾‹')}
- å‰‚é‡æ‰©å±•æœŸï¼š{confirmed_info.get('dose_expansion_n', '10-20ä¾‹')}
- ç»Ÿè®¡å­¦å‡è®¾å’Œè®¡ç®—ä¾æ®

å‚è€ƒçŸ¥è¯†åº“ï¼š
{knowledge_context}

{quality_requirements}
""",

        "ç ”ç©¶äººç¾¤": f"""
è¯·åˆ¶å®š{indication}æ‚£è€…å‚åŠ {drug_type}{study_phase}ä¸´åºŠè¯•éªŒçš„"ç ”ç©¶äººç¾¤"ç« èŠ‚ã€‚

### 3.1 å…¥é€‰æ ‡å‡†
1. å¹´é¾„â‰¥18å²ï¼Œâ‰¤75å²ï¼Œæ€§åˆ«ä¸é™
2. ç»„ç»‡å­¦æˆ–ç»†èƒå­¦ç¡®è¯Šçš„{indication}
3. ç–¾ç—…åˆ†æœŸè¦æ±‚ï¼š{confirmed_info.get('disease_stage', 'å±€éƒ¨æ™šæœŸæˆ–è½¬ç§»æ€§')}
4. æ—¢å¾€æ²»ç–—ï¼š{confirmed_info.get('prior_therapy', 'æ ‡å‡†æ²»ç–—å¤±è´¥æˆ–ä¸è€å—')}
5. ECOG PSè¯„åˆ†ï¼š0-1åˆ†ï¼ˆæˆ–KPSâ‰¥70åˆ†ï¼‰
6. é¢„æœŸç”Ÿå­˜æœŸâ‰¥3ä¸ªæœˆ
7. è‡³å°‘æœ‰ä¸€ä¸ªå¯æµ‹é‡ç—…ç¶ï¼ˆRECIST 1.1æ ‡å‡†ï¼‰
8. å™¨å®˜åŠŸèƒ½æ»¡è¶³ï¼š
   - è¡€æ¶²å­¦ï¼šANCâ‰¥1.5Ã—10â¹/Lï¼ŒPLTâ‰¥100Ã—10â¹/Lï¼ŒHbâ‰¥90g/L
   - è‚åŠŸèƒ½ï¼šTBILâ‰¤1.5Ã—ULNï¼ŒALT/ASTâ‰¤2.5Ã—ULNï¼ˆè‚è½¬ç§»â‰¤5Ã—ULNï¼‰
   - è‚¾åŠŸèƒ½ï¼šCrâ‰¤1.5Ã—ULNæˆ–è‚Œé…æ¸…é™¤ç‡â‰¥50mL/min
   - å¿ƒåŠŸèƒ½ï¼šLVEFâ‰¥50%
9. ç”Ÿè‚²èƒ½åŠ›è€…åŒæ„é‡‡å–æœ‰æ•ˆé¿å­•æªæ–½
10. ç­¾ç½²çŸ¥æƒ…åŒæ„ä¹¦

ç‰¹æ®Šè¦æ±‚ï¼ˆå¦‚é€‚ç”¨ï¼‰ï¼š
- HLAåˆ†å‹ï¼š{confirmed_info.get('hla_requirement', '')}
- ç”Ÿç‰©æ ‡å¿—ç‰©ï¼š{confirmed_info.get('biomarker_requirement', '')}

### 3.2 æ’é™¤æ ‡å‡†
1. ä¸­æ¢ç¥ç»ç³»ç»Ÿè½¬ç§»ï¼ˆé™¤éå·²æ²»ç–—ç¨³å®šâ‰¥4å‘¨ï¼‰
2. æ´»åŠ¨æ€§è‡ªèº«å…ç–«æ€§ç–¾ç—…
3. éœ€è¦ç³»ç»Ÿæ€§å…ç–«æŠ‘åˆ¶æ²»ç–—
4. æ´»åŠ¨æ€§æ„ŸæŸ“ï¼ˆHBVã€HCVã€HIVã€ç»“æ ¸ç­‰ï¼‰
5. æ—¢å¾€ä½¿ç”¨è¿‡{drug_type}ç±»ä¼¼è¯ç‰©
6. 4å‘¨å†…æ¥å—è¿‡å…¶ä»–æŠ—è‚¿ç˜¤æ²»ç–—
7. ä¸¥é‡çš„å¿ƒè¡€ç®¡ç–¾ç—…å²
8. å¦Šå¨ æˆ–å“ºä¹³æœŸå¦‡å¥³
9. ç²¾ç¥ç–¾ç—…æˆ–ä¾ä»æ€§å·®
10. ç ”ç©¶è€…è®¤ä¸ºä¸é€‚åˆå‚åŠ è¯•éªŒçš„å…¶ä»–æƒ…å†µ

### 3.3 é€€å‡ºæ ‡å‡†
- å—è¯•è€…æ’¤å›çŸ¥æƒ…åŒæ„
- ç–¾ç—…è¿›å±•
- ä¸å¯è€å—çš„æ¯’æ€§
- ç ”ç©¶è€…åˆ¤æ–­ç»§ç»­æ²»ç–—å¯¹å—è¯•è€…ä¸åˆ©
- ä¸¥é‡è¿èƒŒæ–¹æ¡ˆ
- å¦Šå¨ 

### 3.4 ä¸­æ­¢æ ‡å‡†
- è¿ç»­å‡ºç°éé¢„æœŸçš„SAE
- DSMBå»ºè®®ç»ˆæ­¢
- ç›‘ç®¡éƒ¨é—¨è¦æ±‚

å‚è€ƒçŸ¥è¯†åº“ï¼š
{knowledge_context}

{quality_requirements}
"""
    }
    
    return prompts.get(module_name, f"è¯·æ’°å†™{module_name}éƒ¨åˆ†çš„å†…å®¹ã€‚{quality_requirements}")

def generate_protocol_with_knowledge_enhancement(module_name, confirmed_info, knowledge_results):
    """ç»“åˆçŸ¥è¯†åº“æ£€ç´¢ç»“æœç”Ÿæˆåè®®å†…å®¹"""
    
    # æ•´ç†çŸ¥è¯†åº“æ£€ç´¢ç»“æœ
    knowledge_context = "\n\n".join([
        f"ã€{result['knowledge_type']}ã€‘\n{result['content']}"
        for result in knowledge_results[:3]  # ä½¿ç”¨æœ€ç›¸å…³çš„å‰3ä¸ªç»“æœ
    ])
    
    # è·å–è¯¥æ¨¡å—çš„ç”Ÿæˆæç¤ºè¯
    prompt = get_module_generation_prompt(module_name, confirmed_info, knowledge_context)
    
    # æ·»åŠ å¼•ç”¨è¦æ±‚
    prompt += """

é‡è¦è¦æ±‚ï¼š
1. å†…å®¹å¿…é¡»åŸºäºå¾ªè¯åŒ»å­¦è¯æ®
2. å¼•ç”¨çš„æ–‡çŒ®å¿…é¡»çœŸå®å¯æŸ¥ï¼ˆæä¾›PMID/DOI/ä¸´åºŠè¯•éªŒæ³¨å†Œå·ï¼‰
3. æ•°æ®å’Œç»“è®ºå¿…é¡»æœ‰æ˜ç¡®æ¥æº
4. ä¸“ä¸šæœ¯è¯­ä½¿ç”¨è§„èŒƒï¼Œå¯åŠ æ³¨è‹±æ–‡
5. æ ¼å¼ç¬¦åˆåŒ»å­¦è®ºæ–‡å†™ä½œè§„èŒƒ
"""
    
    return prompt

@app.post("/generate_protocol_stream")
async def generate_protocol_stream(request: ProtocolStreamRequest):
    """æ­¥éª¤3ï¼šåŸºäºå¤§çº²å®æ—¶ç”Ÿæˆå®Œæ•´åè®®å†…å®¹"""
    from fastapi.responses import StreamingResponse
    import asyncio
    
    async def generate_content():
        try:
            # 1. å…ˆè¿›è¡ŒçŸ¥è¯†åº“æ£€ç´¢å¢å¼º
            knowledge_results = []
            if request.settings.get('include_references', True):
                # åŸºäºç¡®è®¤ä¿¡æ¯æ„å»ºæ£€ç´¢æŸ¥è¯¢
                search_queries = [
                    f"{request.confirmed_info.get('drug_type', '')} {request.confirmed_info.get('indication', '')}",
                    f"{request.confirmed_info.get('study_phase', '')} ä¸´åºŠè¯•éªŒè®¾è®¡",
                    f"{request.confirmed_info.get('indication', '')} å…¥ç»„æ ‡å‡†"
                ]
                
                for query in search_queries:
                    if query.strip():
                        search_result = await search_knowledge_embedding(query, top_k=3)
                        if search_result['success']:
                            knowledge_results.extend(search_result['results'])
            
            # 2. æŒ‰ç…§å¤§çº²é€ä¸ªæ¨¡å—ç”Ÿæˆå†…å®¹
            full_content = ""
            total_sections = len(request.outline)
            
            for idx, section in enumerate(request.outline):
                # è·å–è¯¥æ¨¡å—ç›¸å…³çš„çŸ¥è¯†
                relevant_knowledge = [k for k in knowledge_results 
                                    if any(keyword in k['content'] 
                                          for keyword in section['title'].split())]
                
                # æ„å»ºè¯¥æ¨¡å—çš„ç”Ÿæˆæç¤ºè¯
                module_prompt = generate_protocol_with_knowledge_enhancement(
                    section['title'], 
                    request.confirmed_info,
                    relevant_knowledge[:3]
                )
                
                # è°ƒç”¨LLMç”Ÿæˆè¯¥æ¨¡å—å†…å®¹
                module_content = call_local_llm(module_prompt, temperature=0.3)
                
                # æµå¼è¾“å‡º
                chunk_data = {
                    "content": f"\n## {section['title']}\n\n{module_content}\n",
                    "progress": (idx + 1) / total_sections,
                    "current_module": section['title'],
                    "done": False
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"
                
                full_content += chunk_data["content"]
                await asyncio.sleep(0.1)  # ç»™å‰ç«¯æ—¶é—´æ¸²æŸ“
            
            # 3. è´¨é‡æ£€æŸ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if request.settings.get('include_quality_check', True):
                quality_prompt = f"""
                è¯·å¯¹ä»¥ä¸‹ä¸´åºŠè¯•éªŒæ–¹æ¡ˆè¿›è¡Œè´¨é‡è¯„ä¼°ï¼ŒåŒ…æ‹¬ï¼š
                1. å†…å®¹å®Œæ•´æ€§ï¼ˆå„ç« èŠ‚æ˜¯å¦é½å…¨ï¼‰
                2. ç§‘å­¦ä¸¥è°¨æ€§ï¼ˆè®¾è®¡æ˜¯å¦åˆç†ï¼‰
                3. æ³•è§„åˆè§„æ€§ï¼ˆæ˜¯å¦ç¬¦åˆGCPè¦æ±‚ï¼‰
                4. é€»è¾‘ä¸€è‡´æ€§ï¼ˆå‰åæ˜¯å¦çŸ›ç›¾ï¼‰
                
                æ–¹æ¡ˆå†…å®¹ï¼š
                {full_content[:3000]}...
                
                è¯·ç»™å‡º0-100çš„è¯„åˆ†å’Œæ”¹è¿›å»ºè®®ã€‚
                """
                
                quality_result = call_local_llm(quality_prompt, temperature=0.1)
                
                # è§£æè´¨é‡è¯„åˆ†
                import re
                score_match = re.search(r'(\d+)åˆ†', quality_result)
                quality_score = int(score_match.group(1)) if score_match else 85
                
                quality_data = {
                    "content": f"\n## è´¨é‡è¯„ä¼°æŠ¥å‘Š\n\n{quality_result}\n",
                    "quality_score": quality_score,
                    "done": False
                }
                yield f"data: {json.dumps(quality_data)}\n\n"
            
            # å‘é€å®Œæˆä¿¡å·
            final_data = {
                "content": "",
                "progress": 1.0,
                "done": True,
                "total_length": len(full_content)
            }
            yield f"data: {json.dumps(final_data)}\n\n"
            
        except Exception as e:
            error_data = {
                "error": str(e),
                "done": True
            }
            yield f"data: {json.dumps(error_data)}\n\n"
    
    return StreamingResponse(
        generate_content(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )

@app.post("/export_protocol")
async def export_protocol(request: ExportProtocolRequest):
    """å¯¼å‡ºåè®®ä¸ºä¸åŒæ ¼å¼"""
    try:
        from fastapi.responses import Response
        import io
        
        if request.format.lower() == 'pdf':
            # ç®€åŒ–çš„PDFç”Ÿæˆï¼ˆéœ€è¦å®‰è£…reportlabï¼‰
            try:
                from reportlab.lib.pagesizes import A4
                from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
                from reportlab.lib.styles import getSampleStyleSheet
                
                buffer = io.BytesIO()
                doc = SimpleDocTemplate(buffer, pagesize=A4)
                styles = getSampleStyleSheet()
                story = []
                
                # æ·»åŠ æ ‡é¢˜
                title = Paragraph("åŒ»ç–—æ“ä½œåè®®", styles['Title'])
                story.append(title)
                story.append(Spacer(1, 12))
                
                # æ·»åŠ å†…å®¹
                content = Paragraph(request.content.replace('\n', '<br/>'), styles['Normal'])
                story.append(content)
                
                doc.build(story)
                buffer.seek(0)
                
                return Response(
                    content=buffer.getvalue(),
                    media_type="application/pdf",
                    headers={"Content-Disposition": "attachment; filename=protocol.pdf"}
                )
                
            except ImportError:
                # å¦‚æœæ²¡æœ‰reportlabï¼Œè¿”å›æ–‡æœ¬æ ¼å¼
                return Response(
                    content=request.content.encode('utf-8'),
                    media_type="text/plain",
                    headers={"Content-Disposition": "attachment; filename=protocol.txt"}
                )
        
        elif request.format.lower() == 'docx':
            # ç®€åŒ–çš„Wordæ–‡æ¡£ç”Ÿæˆï¼ˆéœ€è¦å®‰è£…python-docxï¼‰
            try:
                from docx import Document
                
                doc = Document()
                doc.add_heading('åŒ»ç–—æ“ä½œåè®®', 0)
                
                # æ·»åŠ å…ƒæ•°æ®
                if request.metadata:
                    doc.add_heading('åè®®ä¿¡æ¯', level=1)
                    for key, value in request.metadata.get('confirmed_info', {}).items():
                        doc.add_paragraph(f"{key}: {value}")
                
                doc.add_heading('åè®®å†…å®¹', level=1)
                doc.add_paragraph(request.content)
                
                buffer = io.BytesIO()
                doc.save(buffer)
                buffer.seek(0)
                
                return Response(
                    content=buffer.getvalue(),
                    media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    headers={"Content-Disposition": "attachment; filename=protocol.docx"}
                )
                
            except ImportError:
                # å¦‚æœæ²¡æœ‰python-docxï¼Œè¿”å›æ–‡æœ¬æ ¼å¼
                return Response(
                    content=request.content.encode('utf-8'),
                    media_type="text/plain",
                    headers={"Content-Disposition": "attachment; filename=protocol.txt"}
                )
        
        else:  # é»˜è®¤ä¸ºæ–‡æœ¬æ ¼å¼
            return Response(
                content=request.content.encode('utf-8'),
                media_type="text/plain",
                headers={"Content-Disposition": f"attachment; filename=protocol.{request.format}"}
            )
            
    except Exception as e:
        logger.error(f"å¯¼å‡ºåè®®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºå¤±è´¥: {str(e)}")

# å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("ğŸš€ åŒ»å­¦AI Agent APIæœåŠ¡å¯åŠ¨ä¸­...")
    logger.info("âœ… å¸¦çœŸå®LLMè°ƒç”¨çš„APIæœåŠ¡å¯åŠ¨æˆåŠŸ!")
    logger.info("ğŸ“– APIæ–‡æ¡£åœ°å€: http://localhost:8000/docs")
    logger.info("ğŸŒ å‰ç«¯åœ°å€: http://localhost:3000")
    logger.info(f"ğŸ¤– LLMé…ç½®: {current_config['llm']['url']} | {current_config['llm']['model']}")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    logger.info("ğŸ‘‹ åŒ»å­¦AI Agent APIæœåŠ¡æ­£åœ¨å…³é—­...")

if __name__ == "__main__":
    uvicorn.run(
        "start_simple:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    ) 
